%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}
\mode<presentation> {
\usetheme{Madrid}
}
\usepackage{url}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{booktabs}

% for mathematics
\usepackage{amsmath}
\usepackage{amsthm}

% for hollowed 1 (indicator variable symbol)
\usepackage{bbm}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Hawkes Process in Finance]{Hawkes Process Presentation 1} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Dong Shaocong, \\Wang Zexin} % Your name
\institute[NUS]
{
Hawkes Process in Finance\\[3mm]
\medskip
\textit{Quantitative Finance\\
National University of Singapore\\}
}
\date{\today}

\begin{document}
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------
\begin{frame}
\titlepage
\end{frame}

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%------------------------------------------------
\begin{frame}
\frametitle{Today's Agenda}
\tableofcontents
\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Introduction}
\begin{itemize}
	\item stub
\end{itemize}
\end{frame}

%------------------------------------------------
\section{Preliminary} %------------------------------------------------
\begin{frame}
\frametitle{Counting Process / Point Process}
Counting process is the number of arrivals until a certain point of time.\\
Arrivals are following certain prespecified distributions.
\begin{itemize}
	\item Event times: $T_i$ time of the $i$-th event
	\item $N_t = \sum_{i \ge 1} \mathbbm{1}_{\{ t \ge T_i\}}$
	\item $N_0 = 0$
	\item jump size $= 1$ at $t = T_i \; \forall i$
\end{itemize}
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Poisson Process}
Arrival time are distributed as exponential random variables.
\begin{itemize}
	\item $\tau_i \sim Exp(\lambda)$
	\item pdf of $\tau$ : $f_{\tau}(t) = \lambda e^{-\lambda t}, \forall t \ge 0$
	\item expectation of $\tau$ : $\mathbb{E}(\tau) = \lambda^{-1}$
	\item $T_n = \sum_{j=1}^n \tau_j$
	\item $N_t = \sum_{i \ge 1} \mathbbm{1}_{\{ t \ge T_i\}}$
	\item Memoryless property: $\mathbb{P}(\tau > t + m \mid \tau > m) = \mathbb{P}(\tau > t)$
	\item Homogeneous: all arrival times are distributed as exponential random variables with the same $\lambda$
	\item Non-Homogeneous : intensity varies with time, more formally defined as: $\lambda(t \mid H_t) = \lim_{h \to 0} \frac{\mathbb{P}\{ N_{t + h} - N_t = 1 \mid H_t\}}{h}$
\end{itemize}
\end{frame}


\section{Univariate Hawkes Process}

\begin{frame}
\frametitle{Hawkes Process}
Formulation:\\
$\lambda(t \mid H_t) = \lambda_0(t) + \sum_{i: t > T_i} \phi(t - T_i)$
\begin{itemize}
	\item Self-exciting property: Arrivals of events increase the likelihood of future observations. Intensity at the current time depends on how many and how far away are the most recent arrivals in the past.
	\item Non-Homogeneous, as the intensity of the exponential random variables changes with time.
	\item Deterministic base intensity function: $\lambda_0(t)$
	\item Memory Kernel: $\phi(t - T_i)$ which links to past arrivals times
	\item Event Decay: monotonically decreasing kernel
\end{itemize}
\end{frame}

\subsection{Branching Structure}
\begin{frame}
\frametitle{Branching Structure}
\begin{itemize}
	\item We can divide the events into two categories, immigrants and the offsprings.
	\item Immigrants : the events directly caused by the base intensity
	\item Offsprings : the events `\textbf{excited}' by immigrants or another offspring
	\item Self-exciting is the property which can be observed in the financial markets as momentum, in which investors following the trend in the prices tend to cause trend in the same direction.
	\item In econometrics, this property of markets is observed as reverse casuality, as price movements drive demands, and demands drive prices as well.
\end{itemize}
TODO : one image of branching structure here - univariate case
\end{frame}

\subsection{Deterministic Intensity Functions}
\begin{frame}
\frametitle{Deterministic Intensity Functions}
Intensity function $\lambda_0(t)>0$, describes external event triggering
\begin{itemize}
	\item Independence: previous events within the process
	\item Base (or background) intensity
	\item In the multivariate/bivariate case, the intensity function can be a constant.
	\item For example, $\lambda_i,0(t) = \mu_i$ for a formulation of $\lambda_i(t) = \mu_i + \sum_{i: t > T_i} \phi(t - T_i)$
\end{itemize}
\end{frame}



\subsection{Cluster of offspring}
\begin{frame}
\frametitle{Cluster of offspring}
\begin{itemize}
	\item The offsprings of one immigrant events, which are the immediate offsprings of it, and their immediate offsprings, and their immediate offsprings, \dots, can be grouped as one cluster.
	\item Branching factor is defined as the expected number of events generated by a parent event : $\mid \Phi \mid = \int_0^{\infty} \phi(\tau) d\tau$
	\item Sub-Critical phase if $\mid \Phi \mid < 1$, meaning the number of events in one cluster is bounded.
	\item Super-Critical if $\mid \Phi \mid > 1$, meaning the number of events in one cluster is unbounded.
	\item The properties of Sub-Critical and Super-Critical mimics those in 
\end{itemize}
\end{frame}

\subsection{Memory Kernels}
\begin{frame}
\frametitle{Memory Kernels}
Memory Kernels can be in any form, with two popular forms:
\begin{itemize}
	\item Exponential decay kernel: $\phi(x) = \alpha e^{-\delta x}$
	\item Power law kernel: $\phi(x) = \frac{\alpha}{(x + \delta)^{\eta + 1}}$
	\item By the self-exciting property, the kernel shoud possess decay property since we expect the intensity to be higher when there are more arrivals in the near past.
\end{itemize}
\end{frame}



%------------------------------------------------
\subsection{Simulation}

\begin{frame}
\frametitle{Thinning Algorithm}
\begin{itemize}
	\item stub
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Decomposition Algorithm}
\begin{itemize}
	\item stub
\end{itemize}
\end{frame}

%------------------------------------------------
\subsection{Parameter Estimation}
\begin{frame}
\frametitle{Likelihood functions}
In this section we put some efforts into writing out the likelihood function of Univariate Hawkes Process, and solving for its parameters if possible and not too computationally costly.
\begin{itemize}
	\item Ideally the likehood function should be a function of the probability density functions at the event times, and the cumulative density functions between event times.
	\item $L(\Theta) = \prod_{i=1}^n f(T_{i}) \prod_{i=1}^n (1-F_{T_{i-1}}(T_i)) (1-F_{T_{n}}(T))$
	\item Cumulative density function for non-homogeneous Poisson Process can written as : $F(t) = 1 - \exp(-\int_0^t \lambda(s) ds)$, where the intensity $\lambda(s)$ is a function of time
	\item Probability density function for Hawkes Process, as we described in the previous slides, can be written as : $f(t) = \lambda(t) \exp(t\lambda(t))$, however when we let it participate in the likelihood function, $f(t) = \lambda(t)$ since the duration of event is negligible.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Likelihood functions}
\begin{equation*}
\begin{split}
L(\Theta) = &\prod_{i=1}^n f(T_{i}) \prod_{i=1}^n (1-F_{T_{i-1}}(T_i)) (1-F_{T_{n}}(T))\\
= &[\prod_{i=1}^n \lambda(T_{i})] [\prod_{i=1}^n \exp(-\int_{T_{i-1}}^{T_i} \lambda(s) ds)] (\exp(-\int_{T_{n}}^{T} \lambda(s) ds))\\
= &\exp(-\int_{0}^{T} \lambda(s) ds) \prod_{i=1}^n \lambda(T_{i})
\end{split}
\end{equation*}
\end{frame}

\begin{frame}
\frametitle{Likelihood functions}
Suppose we are going to use exponential decay kernel representation and constant base intensity function:\\
$\phi(t) = \alpha e^{\delta t}$, $\lambda_0(t) = \mu \to \lambda(t \mid F_{t}) = \mu + \sum_{i: t>T_i} \alpha e^{\delta (t - T_{i})}$
\begin{equation*}
\begin{split}
L(\Theta) = &\exp(-\int_{0}^{T} \lambda(t \mid F_t) dt) \prod_{i=1}^n \lambda(T_i \mid F_{T_{i}})\\
= &\exp(-\int_{0}^{T}  \mu + \sum_{i: t>T_i} \alpha e^{\delta (t - T_i)} dt) \prod_{i=1}^n (\mu + \sum_{j=0}^i \alpha e^{\delta (T_i - T_j)})\\[5mm]
\ln L(\Theta) = & \sum_{i=1}^n \ln (\mu + \alpha \sum_{j=0}^i  e^{\delta (T_i - T_j)}) - \mu T - \alpha \int_{0}^{T}  \sum_{i: t>T_i} e^{\delta (t - T_i)} dt 
\end{split}
\end{equation*}
\end{frame}

\begin{frame}
\frametitle{Maximum Likelihood Estimates}
Differentiate with respect to $\mu$:
\begin{equation*}
\begin{split}
\ln L(\Theta) = & \sum_{i=1}^n \ln (\mu + \alpha \sum_{j=0}^i  e^{\delta (T_i - T_j)}) - \mu T - \alpha \int_{0}^{T}  \sum_{i: t>T_i} e^{\delta (t - T_i)} dt \\
\frac{\partial}{\partial \mu} \ln L(\Theta) =& \sum_{i=1}^n \frac{1}{\mu + \alpha \sum_{j=0}^i e^{\delta (T_i - T_j)}} - T = 0\\
\to &\sum_{i=1}^n \frac{1}{\mu + \alpha \sum_{j=0}^i e^{\delta (T_i - T_j)}} = T
\end{split}
\end{equation*}
\end{frame}

\begin{frame}
\frametitle{Maximum Likelihood Estimates}
Differentiate with respect to $\alpha$:
\begin{equation*}
\begin{split}
\ln L(\Theta) = & \sum_{i=1}^n \ln (\mu + \alpha \sum_{j=0}^i  e^{\delta (T_i - T_j)}) - \mu T - \alpha \int_{0}^{T}  \sum_{i: t>T_i} e^{\delta (t - T_i)} dt \\
\frac{\partial}{\partial \alpha} \ln L(\Theta) =& \sum_{i=1}^n \frac{e^{\delta (T_i - T_j)}}{\mu + \alpha \sum_{j=0}^i e^{\delta (T_i - T_j)}} - \int_{0}^{T}  \sum_{i: t>T_i} e^{\delta (t - T_i)} dt= 0\\
\to& \sum_{i=1}^n \frac{e^{\delta (T_i - T_j)}}{\mu + \alpha \sum_{j=0}^i e^{\delta (T_i - T_j)}} = \int_{0}^{T}  \sum_{i: t>T_i} e^{\delta (t - T_i)} dt
\end{split}
\end{equation*}
\end{frame}

\begin{frame}
\frametitle{Maximum Likelihood Estimates}
Differentiate with respect to $\delta$:
\begin{equation*}
\begin{split}
\ln L(\Theta) = & \sum_{i=1}^n \ln (\mu + \alpha \sum_{j=0}^i  e^{\delta (T_i - T_j)}) - \mu T - \alpha \int_{0}^{T}  \sum_{i: t>T_i} e^{\delta (t - T_i)} dt \\
\frac{\partial}{\partial \delta} \ln L(\Theta) =& \sum_{i=1}^n \frac{(T_i - T_j)e^{\delta (T_i - T_j)}}{\mu + \alpha \sum_{j=0}^i e^{\delta (T_i - T_j)}} - \int_{0}^{T}  \sum_{i: t>T_i} (t - T_i)e^{\delta (t - T_i)} dt= 0\\
\to& \sum_{i=1}^n \frac{(T_i - T_j)e^{\delta (T_i - T_j)}}{\mu + \alpha \sum_{j=0}^i e^{\delta (T_i - T_j)}} = \int_{0}^{T}  \sum_{i: t>T_i} (t - T_i)e^{\delta (t - T_i)} dt
\end{split}
\end{equation*}
\end{frame}

%------------------------------------------------
\begin{frame}
\Huge{\centerline{Thank You}}
\begin{center}
\begin{normalsize}
\emph{E0007424@u.nus.edu}\\
\emph{E0012680@u.nus.edu}
\end{normalsize}
\end{center}
\end{frame}


%------------------------------------------------

\end{document} 